{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c8a1d7-554a-4ccf-b4d8-85c10e74841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-cifar100'...\n",
      "remote: Enumerating objects: 1188, done.\u001b[K\n",
      "remote: Total 1188 (delta 0), reused 0 (delta 0), pack-reused 1188\u001b[K\n",
      "Receiving objects: 100% (1188/1188), 530.69 KiB | 383.00 KiB/s, done.\n",
      "Resolving deltas: 100% (753/753), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/weiaicunzai/pytorch-cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67214e6-417e-4105-bd44-3101473a4826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/jcyuan-gpu/code/fedml-pipeline/pytorch-cifar100\n"
     ]
    }
   ],
   "source": [
    "%cd pytorch-cifar100/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd402c5-cc1e-43bb-ac73-e3af49888e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import resnet18\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6233d445-c196-43d9-b2a3-e0c7fa033deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, dataset, shuffle, batch_size=128):\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "    def set_more_hyperp(self, lr, mom, wd):\n",
    "        self.LEARNING_RATE = lr\n",
    "        self.MOMENTUM = mom\n",
    "        self.WEIGHT_DECAY = wd\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = resnet18()\n",
    "        self.model = nn.DataParallel(self.model).cuda()\n",
    "    def load_model(self, other_node):\n",
    "        self.model = resnet18()\n",
    "        self.model.load_state_dict(other_node.model.module.state_dict())\n",
    "        self.model = nn.DataParallel(self.model).cuda()\n",
    "    def aggregate_then_load_model(self, other_nodes):\n",
    "        params = [node.model.module.state_dict() for node in other_nodes]\n",
    "        self.model = resnet18()\n",
    "        param_master = dict(self.model.state_dict())\n",
    "        for name in param_master:\n",
    "            param_master[name] = 0\n",
    "            for p in params:\n",
    "                param_master[name] += p[name] * (1.0 / len(other_nodes))\n",
    "        self.model.load_state_dict(param_master)\n",
    "        self.model = nn.DataParallel(self.model).cuda()\n",
    "    def save_model(self, path):\n",
    "      torch.save(self.model.module.state_dict(), path)\n",
    "    def load_saved_model(self, path):\n",
    "      weights = torch.load(path)\n",
    "      self.model = resnet18()\n",
    "      self.model.load_state_dict(weights)\n",
    "      self.model = nn.DataParallel(self.model).cuda()\n",
    "\n",
    "    class AverageMeter(object):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "        def reset(self):\n",
    "            self.val = 0\n",
    "            self.avg = 0\n",
    "            self.sum = 0\n",
    "            self.count = 0\n",
    "        def update(self, val, n=1):\n",
    "            self.val = val\n",
    "            self.sum += val * n\n",
    "            self.count += n\n",
    "            self.avg = self.sum / self.count\n",
    "    def accuracy(self, output, target, topk=(1,)):\n",
    "        \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "    def train_model(self, which_epoch):\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        optimizer = optim.SGD(self.model.parameters(), self.LEARNING_RATE, momentum=self.MOMENTUM, weight_decay=self.WEIGHT_DECAY)\n",
    "\n",
    "        batch_time = self.AverageMeter()\n",
    "        data_time = self.AverageMeter()\n",
    "        losses = self.AverageMeter()\n",
    "        top1 = self.AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.dataloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = self.model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = self.accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "        \n",
    "        return {\n",
    "            'epoch': which_epoch,\n",
    "            'batch_time.avg': batch_time.avg,\n",
    "            'data_time.avg': data_time.avg,\n",
    "            'losses.avg': losses.avg,\n",
    "            'top1.avg': top1.avg}\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        batch_time = self.AverageMeter()\n",
    "        losses = self.AverageMeter()\n",
    "        top1 = self.AverageMeter()\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        self.model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(self.dataloader):\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "\n",
    "                # compute output\n",
    "                output = self.model(input)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # measure accuracy and record loss\n",
    "                prec = self.accuracy(output, target)[0]\n",
    "                losses.update(loss.item(), input.size(0))\n",
    "                top1.update(prec.item(), input.size(0))\n",
    "\n",
    "                # measure elapsed time\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "\n",
    "        return {\n",
    "            'batch_time.avg': batch_time.avg,\n",
    "            'losses.avg': losses.avg,\n",
    "            'top1.avg': top1.avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83c4868-a926-4f95-b595-19bd09bedf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a739ac2758a34b4facb2f46b247e71d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "CIFAR100_TRAIN_MEAN = [0.5070751592371323, 0.48654887331495095, 0.4409178433670343]\n",
    "CIFAR100_TRAIN_STD = [0.2673342858792401, 0.2564384629170883, 0.27615047132568404]\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "  root='./data',\n",
    "  train=True,\n",
    "  download=True,\n",
    "  transform=transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "    ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "  root='./data',\n",
    "  train=False,\n",
    "  download=False,\n",
    "  transform=transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "  ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52feb528-03ee-495d-8373-96c569169690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Simulator:\n",
    "#     def __init__(self, seed, trainDataList, testData, clientsPerRound, epochsPerClient, totalEpochs, fn_SGDHyperparams):\n",
    "#         pass\n",
    "#     def __trainSingleModel():\n",
    "#         pass\n",
    "#     def __trainFederatedModel():\n",
    "#         pass\n",
    "#     def evalute():\n",
    "#         pass\n",
    "#     def training():\n",
    "#         pass\n",
    "\n",
    "def train(modelWeights, batchSize, momentum, weightDecay, learningRate, numIterations, trainingData):\n",
    "    model = resnet18()\n",
    "    model.load_state_dict(modelWeights)\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    dataloader = torch.utils.data.DataLoader(trainingData, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.SGD(self.model.parameters(), self.LEARNING_RATE, momentum=self.MOMENTUM, weight_decay=self.WEIGHT_DECAY)\n",
    "\n",
    "    return model.module.state_dict() # does it get off the GPU - one way to find out, run 10K clients\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.dataloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = self.model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = self.accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "        \n",
    "        return {\n",
    "            'epoch': which_epoch,\n",
    "            'batch_time.avg': batch_time.avg,\n",
    "            'data_time.avg': data_time.avg,\n",
    "            'losses.avg': losses.avg,\n",
    "            'top1.avg': top1.avg}\n",
    "    \n",
    "        def accuracy(self, output, target, topk=(1,)):\n",
    "        \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57396a96-f90d-42fe-93dc-79905c6e94e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
